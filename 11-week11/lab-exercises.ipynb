{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the entire set of exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, load_breast_cancer, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Voting Classifiers\n",
    "\n",
    "In this exercise, you'll compare hard and soft voting using different base classifiers\n",
    "on a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.825\n",
      "SVC 0.935\n",
      "KNeighborsClassifier 0.9\n",
      "VotingClassifier 0.915\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "# Generate a moderately complex dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
    "                         n_redundant=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Base classifiers\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "svc_clf = SVC(probability=True, random_state=42)\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "# TODO: Create a VotingClassifier with hard voting\n",
    "# YOUR CODE HERE\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('svc', svc_clf), ('knn', knn_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_hard.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Create a VotingClassifier with soft voting\n",
    "# YOUR CODE HERE\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('svc', svc_clf), ('knn', knn_clf)],\n",
    "    voting='soft')\n",
    "voting_soft.fit(X_train, y_train)\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# TODO: Train and evaluate each classifier (individual and voting)\n",
    "# Add results to the results dictionary\n",
    "# YOUR CODE HERE\n",
    "for clf in (log_clf, svc_clf, knn_clf, voting_hard, voting_soft):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Which voting method performed better? Why do you think this is the case?\n",
    "2. How did the ensemble methods compare to individual classifiers?\n",
    "3. Try adding or removing classifiers - how does this affect performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Bagging Classifier Exploration\n",
    "\n",
    "This exercise explores how bagging parameters affect model performance using the breast cancer dataset. We'll use decision trees as our base estimator and explore how different bagging parameters affect the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 54 (3421769474.py, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 57\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.figure(figsize=(12, 5))\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 54\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer.data, cancer.target\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_cancer, y_cancer, \n",
    "                                                            test_size=0.2, \n",
    "                                                            random_state=42)\n",
    "\n",
    "def evaluate_bagging(n_estimators, max_samples, max_depth=None):\n",
    "    \"\"\"\n",
    "    Creates and evaluates a bagging classifier using decision trees\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators : int\n",
    "        The number of decision trees in the ensemble\n",
    "    max_samples : float\n",
    "        The fraction of samples to draw for training each decision tree\n",
    "    max_depth : int or None\n",
    "        The maximum depth of each decision tree\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (train_score, test_score)\n",
    "    \"\"\"\n",
    "    # Create base decision tree\n",
    "    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    \n",
    "    # TODO: Create a BaggingClassifier using the decision tree\n",
    "    # Use n_jobs=-1 to utilize all CPU cores\n",
    "    # YOUR CODE HERE\n",
    "    bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            max_samples=100, n_jobs=-1, random_state=42)\n",
    "    bagging.fit(X_train, y_train)\n",
    "    train_score = bagging.score(X_train_c, y_train_c)\n",
    "    test_score = bagging.score(X_test_c, y_test_c)\n",
    "   \n",
    "    \n",
    "    # TODO: Fit the model and compute training and test scores\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return train_score, test_score\n",
    "\n",
    "# Test different combinations\n",
    "sample_sizes = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "n_estimators_list = [10, 50, 100, 200]\n",
    "\n",
    "# Create arrays to store results\n",
    "train_scores = np.zeros((len(sample_sizes), len(n_estimators_list)))\n",
    "test_scores = np.zeros((len(sample_sizes), len(n_estimators_list)))\n",
    "\n",
    "# TODO: Fill in the arrays with scores for each parameter combination\n",
    "# YOUR CODE HERE\n",
    "for i, sample_sizes in enumerate(sample_sizes):\n",
    "    for j, n_est in enumerate(n_estimators_list):\n",
    "        \n",
    "# Visualize results with a heatmap\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Training scores heatmap\n",
    "plt.subplot(121)\n",
    "plt.imshow(train_scores, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Training Accuracy')\n",
    "plt.xticks(range(len(n_estimators_list)), n_estimators_list)\n",
    "plt.yticks(range(len(sample_sizes)), sample_sizes)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Sample Size Fraction')\n",
    "plt.title('Training Accuracy')\n",
    "\n",
    "# Test scores heatmap\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_scores, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Test Accuracy')\n",
    "plt.xticks(range(len(n_estimators_list)), n_estimators_list)\n",
    "plt.yticks(range(len(sample_sizes)), sample_sizes)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Sample Size Fraction')\n",
    "plt.title('Test Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bonus investigation: Effect of tree depth\n",
    "# TODO: Create a similar analysis varying max_depth instead of max_samples\n",
    "# Try max_depth values of [3, 5, 7, None]\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. How does the number of trees affect model performance? Is there a point of diminishing returns?\n",
    "2. What effect does sample size have on overfitting/underfitting? Look at the difference\n",
    "   between training and test scores.\n",
    "3. Why might using a very small sample size (0.1) be problematic? What about using 1.0?\n",
    "4. How does the performance of the bagging classifier compare to a single decision tree?\n",
    "5. For the bonus investigation: how does tree depth affect the benefits of bagging?\n",
    "\n",
    "**Additional Challenge:**\n",
    "\n",
    "1. Add oob_score=True to the BaggingClassifier and compare the OOB score with the test score.\n",
    "   How well does the OOB score estimate generalization performance?\n",
    "2. Try using bootstrap=False and compare the results with the default bootstrap=True.\n",
    "   What differences do you observe and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Random Forest Parameter Tuning\n",
    "\n",
    "This exercise explores Random Forest hyperparameters using the California housing dataset.  Note, in this problem, you'll want to use a random forest regressor, rather than a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1.03  3.821 1.726 ... 2.221 2.835 3.25 ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m features \u001b[38;5;129;01min\u001b[39;00m max_features:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n_est \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m]:\n\u001b[0;32m---> 28\u001b[0m             train_mse, test_mse \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_rf_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_est\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m             results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     30\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mstr\u001b[39m(depth),\n\u001b[1;32m     31\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mstr\u001b[39m(features),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_mse\u001b[39m\u001b[38;5;124m'\u001b[39m : test_mse\n\u001b[1;32m     35\u001b[0m             })\n\u001b[1;32m     36\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m, in \u001b[0;36mevaluate_rf_params\u001b[0;34m(n_estimators, max_depth, max_features)\u001b[0m\n\u001b[1;32m     14\u001b[0m rf\u001b[38;5;241m.\u001b[39mfit(X_train_h, y_train_h)\n\u001b[1;32m     15\u001b[0m train_mse \u001b[38;5;241m=\u001b[39m root_mean_squared_error(y_train_h, rf\u001b[38;5;241m.\u001b[39mpredict(X_train_h))\n\u001b[0;32m---> 16\u001b[0m test_mse \u001b[38;5;241m=\u001b[39m root_mean_squared_error(y_test_h, \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_h\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_mse, test_mse\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:1063\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1061\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:641\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    639\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:1050\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1045\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1046\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1047\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1048\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m             )\n\u001b[0;32m-> 1050\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1.03  3.821 1.726 ... 2.221 2.835 3.25 ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_house, y_house = housing.data, housing.target\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_house, y_house, \n",
    "                                                            test_size=0.2, \n",
    "                                                            random_state=42)\n",
    "\n",
    "def evaluate_rf_params(n_estimators, max_depth, max_features):\n",
    "    rf= RandomForestRegressor(\n",
    "        n_estimators = n_estimators,\n",
    "        max_depth = max_depth,\n",
    "        max_features=max_features,\n",
    "        random_state = 42\n",
    "    )\n",
    "    rf.fit(X_train_h, y_train_h)\n",
    "    train_mse = root_mean_squared_error(y_train_h, rf.predict(X_train_h))\n",
    "    test_mse = root_mean_squared_error(y_test_h, rf.predict(y_train_h))\n",
    "    \n",
    "    return train_mse, test_mse\n",
    "\n",
    "# Parameter combinations to test\n",
    "max_depths = [5, 10, 15, None]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "\n",
    "results = []\n",
    "for depth in max_depths:\n",
    "    for features in max_features:\n",
    "        for n_est in [50, 100, 200]:\n",
    "            train_mse, test_mse = evaluate_rf_params(n_est, depth, features)\n",
    "            results.append({\n",
    "                'max_depth' : str(depth),\n",
    "                'max_features' : str(features),\n",
    "                'n_estimators' : n_est,\n",
    "                'train_mse' : train_mse,\n",
    "                'test_mse' : test_mse\n",
    "            })\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. How does max_depth affect the bias-variance tradeoff?\n",
    "2. What's the impact of different max_features settings?\n",
    "3. At what point do additional trees stop improving performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Feature Importance Analysis\n",
    "\n",
    "This exercise explores feature importance in Random Forests using both the breast cancer \n",
    "and housing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# TODO: Train Random Forests on both datasets and visualize feature importance\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m     14\u001b[0m rf_cancer \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m rf_cancer\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_c\u001b[49m, y_train_c)\n\u001b[1;32m     17\u001b[0m rf_house \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     18\u001b[0m rf_house\u001b[38;5;241m.\u001b[39mfit(X_train_h, y_train_h)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_c' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_feature_importance(rf_model, feature_names, title):\n",
    "    importances = rf_model.feature_importances_\n",
    "    sorted_idx = np.argsort(importances)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.barh(range(X.shape[1]), importances[sorted_idx], align='center')\n",
    "    plt.yticks(range(X.shape[1]), feature_names)\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Feature Importance using Random Forest')\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Train Random Forests on both datasets and visualize feature importance\n",
    "# YOUR CODE HERE\n",
    "rf_cancer = RandomForestClassifier(n_estimators=100)\n",
    "rf_cancer.fit(X_train_c, y_train_c)\n",
    "\n",
    "rf_house = RandomForestClassifier(n_estimators=100)\n",
    "rf_house.fit(X_train_h, y_train_h)\n",
    "\n",
    "plot_feature_importance(rf_cancer, cancer.feature_names, \"Breast Cancer\")\n",
    "plot_feature_importance(rf.house, housing.feature_names, \"Housing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Which features are most important for each dataset?\n",
    "2. How stable are the importance rankings across multiple runs?\n",
    "3. How could you use this information for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Out-of-Bag Evaluation\n",
    "\n",
    "This exercise compares Out-of-Bag (OOB) evaluation with cross-validation for error \n",
    "estimation in Random Forests. You'll see how these different evaluation methods compare\n",
    "and when you might want to use each one.\n",
    "\n",
    "We'll use both the breast cancer dataset (classification) and a simpler synthetic\n",
    "dataset to understand how these evaluation methods work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "# Create a synthetic dataset for initial exploration\n",
    "X_simple, y_simple = make_classification(n_samples=1000, n_features=10, \n",
    "                                       n_informative=5, random_state=42)\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer.data, cancer.target\n",
    "\n",
    "def get_oob_scores(X, y, n_estimators_range, random_state=42):\n",
    "    \"\"\"\n",
    "    Calculate OOB scores for different numbers of estimators.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Training data\n",
    "    y : array-like\n",
    "        Target values\n",
    "    n_estimators_range : array-like\n",
    "        Range of n_estimators values to test\n",
    "    random_state : int\n",
    "        Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (oob_scores, computation_times)\n",
    "    \"\"\"\n",
    "    oob_scores = []\n",
    "    computation_times = []\n",
    "    \n",
    "    for n_estimators in n_estimators_range:\n",
    "        # TODO: Create a RandomForestClassifier with oob_score=True\n",
    "        # Use n_jobs=-1 to utilize all CPU cores\n",
    "        rf = None  # YOUR CODE HERE\n",
    "        \n",
    "        # Time the fitting process\n",
    "        start_time = time.time()\n",
    "        # TODO: Fit the random forest\n",
    "        # YOUR CODE HERE\n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        # TODO: Get the oob score and append it to oob_scores\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        computation_times.append(computation_time)\n",
    "    \n",
    "    return np.array(oob_scores), np.array(computation_times)\n",
    "\n",
    "def get_cv_scores(X, y, n_estimators_range, cv=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Calculate cross-validation scores for different numbers of estimators.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Training data\n",
    "    y : array-like\n",
    "        Target values\n",
    "    n_estimators_range : array-like\n",
    "        Range of n_estimators values to test\n",
    "    cv : int\n",
    "        Number of cross-validation folds\n",
    "    random_state : int\n",
    "        Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (cv_scores_mean, cv_scores_std, computation_times)\n",
    "    \"\"\"\n",
    "    cv_scores_mean = []\n",
    "    cv_scores_std = []\n",
    "    computation_times = []\n",
    "    \n",
    "    for n_estimators in n_estimators_range:\n",
    "        # TODO: Create a RandomForestClassifier (without OOB)\n",
    "        rf = None  # YOUR CODE HERE\n",
    "        \n",
    "        # Time the cross-validation process\n",
    "        start_time = time.time()\n",
    "        # TODO: Perform cross-validation and get scores\n",
    "        # Hint: use cross_val_score from sklearn\n",
    "        scores = None  # YOUR CODE HERE\n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        # TODO: Calculate and store mean and std of CV scores\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        computation_times.append(computation_time)\n",
    "    \n",
    "    return (np.array(cv_scores_mean), np.array(cv_scores_std), \n",
    "            np.array(computation_times))\n",
    "\n",
    "# Test range of estimators\n",
    "n_estimators_range = [10, 20, 50, 100, 200]\n",
    "\n",
    "def plot_comparison(X, y, title):\n",
    "    \"\"\"\n",
    "    Plot OOB scores vs CV scores and computation times.\n",
    "    \"\"\"\n",
    "    # Get scores and computation times\n",
    "    oob_scores, oob_times = get_oob_scores(X, y, n_estimators_range)\n",
    "    cv_scores_mean, cv_scores_std, cv_times = get_cv_scores(X, y, n_estimators_range)\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot scores\n",
    "    ax1.plot(n_estimators_range, oob_scores, 'b-', label='OOB Score')\n",
    "    ax1.plot(n_estimators_range, cv_scores_mean, 'r-', label='CV Score')\n",
    "    ax1.fill_between(n_estimators_range, \n",
    "                    cv_scores_mean - cv_scores_std,\n",
    "                    cv_scores_mean + cv_scores_std, \n",
    "                    alpha=0.2, color='r')\n",
    "    ax1.set_xlabel('Number of Estimators')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title(f'OOB vs CV Scores - {title}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot computation times\n",
    "    ax2.plot(n_estimators_range, oob_times, 'b-', label='OOB Time')\n",
    "    ax2.plot(n_estimators_range, cv_times, 'r-', label='CV Time')\n",
    "    ax2.set_xlabel('Number of Estimators')\n",
    "    ax2.set_ylabel('Computation Time (seconds)')\n",
    "    ax2.set_title(f'Computation Times - {title}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Run comparisons on both datasets\n",
    "# Compare OOB and CV on simple synthetic dataset\n",
    "plot_comparison(X_simple, y_simple, \"Synthetic Dataset\")\n",
    "\n",
    "# Compare OOB and CV on breast cancer dataset\n",
    "plot_comparison(X_cancer, y_cancer, \"Breast Cancer Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. How do the OOB and CV scores compare? Are they similar or different?\n",
    "   - Look at both the mean values and the trends as n_estimators increases\n",
    "   - Which method gives more optimistic/pessimistic estimates?\n",
    "\n",
    "2. Compare the computation times:\n",
    "   - Which method is faster? By how much?\n",
    "   - How does the difference in computation time scale with n_estimators?\n",
    "   - Why might one method be faster than the other?\n",
    "\n",
    "3. Looking at the synthetic vs real dataset:\n",
    "   - Are the results similar or different?\n",
    "   - Which dataset shows more variance in the scores?\n",
    "   - Can you explain any differences you observe?\n",
    "\n",
    "4. Practical considerations:\n",
    "   - When might you prefer to use OOB evaluation?\n",
    "   - When might cross-validation be a better choice?\n",
    "   - What are the advantages and disadvantages of each method?\n",
    "\n",
    "**Additional Challenges:**\n",
    "\n",
    "1. Modify the code to test different cross-validation fold numbers (cv parameter).\n",
    "   How does this affect both the scores and computation time?\n",
    "\n",
    "2. Add error bars to the OOB scores by running multiple times with different\n",
    "   random states. How stable are the OOB scores compared to CV scores?\n",
    "\n",
    "3. Try different parameters for the RandomForestClassifier (e.g., max_depth,\n",
    "   max_features). How do these affect the comparison between OOB and CV scores?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
